Approach 1 – Make the head return logits + switch to BCEWithLogitsLoss (+ pos_weight)_gradingclapping
We modified the baseline by having the BART classification head return raw logits instead of applying a sigmoid inside the model. This allows us to use BCEWithLogitsLoss, which is numerically more stable than applying a separate sigmoid + BCELoss. To address label imbalance across the 26 paraphrase types, we also introduced a pos_weight vector computed from the training data. This weighting penalizes false negatives more heavily for underrepresented labels, improving the model’s robustness on rare paraphrase types.